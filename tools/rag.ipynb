{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from second_brain.config import settings\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb.retrievers import (\n",
    "    MongoDBAtlasParentDocumentRetriever,\n",
    ")\n",
    "\n",
    "from second_brain.application.rag import get_splitter\n",
    "from second_brain.application.rag.embeddings import EmbeddingModelBuilder\n",
    "\n",
    "embedding_model = EmbeddingModelBuilder().get_model()\n",
    "parent_doc_retriever = MongoDBAtlasParentDocumentRetriever.from_connection_string(\n",
    "    connection_string=settings.MONGODB_URI,\n",
    "    embedding_model=embedding_model,\n",
    "    child_splitter=get_splitter(200),\n",
    "    parent_splitter=get_splitter(800),\n",
    "    database_name=settings.MONGODB_DATABASE_NAME,\n",
    "    collection_name=\"rag_data\",\n",
    "    text_key=\"page_content\",\n",
    "    search_kwargs={\"k\": 10},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Retrieve and parse documents\n",
    "retrieve = {\n",
    "    \"context\": parent_doc_retriever\n",
    "    | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs])),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "}\n",
    "template = \"\"\"Answer the question based only on the following context. If no context is provided, respond with I DON'T KNOW: \\\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "# Define the chat prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "# Define the model to be used for chat completion\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-2024-11-20\")\n",
    "# Parse output as a string\n",
    "parse_output = StrOutputParser()\n",
    "# Naive RAG chain\n",
    "rag_chain = retrieve | prompt | llm | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To optimize LLMs for inference, you can use the following techniques:\n",
      "\n",
      "1. **Lower Precision (Quantization):** Operate at reduced numerical precision, such as 8-bit or 4-bit, to achieve computational advantages without a significant decline in model performance. This reduces memory requirements and speeds up inference.\n",
      "\n",
      "2. **Flash Attention:** Use Flash Attention, a memory-efficient variation of the attention algorithm, which optimizes GPU memory utilization by relying on faster on-chip memory (SRAM) instead of slower VRAM. It provides mathematically identical outputs while being faster and more memory-efficient.\n",
      "\n",
      "3. **Architectural Innovations:** Leverage specialized model architectures designed for efficient inference in autoregressive text generation. Examples include Alibi, Rotary embeddings, Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). These innovations improve the handling of long input contexts and reduce computational overhead.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\"How can I optimize LLMs for inference?\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF stands for Reinforcement Learning from Human Feedback. It is a technique used to fine-tune models, such as language models (LMs), by incorporating human feedback to guide the learning process. This feedback can take the form of preferences, scores, or other evaluative signals provided by humans, which are used to train a reward model. The reward model then helps optimize the policy of the model using reinforcement learning algorithms like PPO (Proximal Policy Optimization). RLHF has been applied to improve the performance of large language models by aligning their outputs more closely with human preferences and values.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\"What is RLHF?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I DON'T KNOW\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\"What is RAGAS?\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
