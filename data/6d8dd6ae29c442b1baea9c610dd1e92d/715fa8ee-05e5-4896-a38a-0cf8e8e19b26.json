{
    "page_metadata": {
        "id": "edfce1dda8983b95f60e6857ffcbff65",
        "url": "https://www.notion.so/Components-Architecture-and-System-Design-edfce1dda8983b95f60e6857ffcbff65",
        "title": "Components, Architecture, and System Design",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "# Notes\n\n\n\n<child_page>\n# ML Pipeline Architecture Design Patterns (With 10 Real-World Examples)\n\n[https://neptune.ai/blog/ml-pipeline-architecture-design-patterns](https://neptune.ai/blog/ml-pipeline-architecture-design-patterns)\n\n# Summary\n\nThis article explores ML pipeline architecture design patterns, highlighting the distinction between ML pipeline architecture and design. It emphasizes the importance of following these patterns for efficiency, scalability, reproducibility, and standardization. The article outlines key stages in ML pipelines, including data ingestion, preprocessing, feature engineering, model training, evaluation, deployment, and monitoring. It then delves into 10 ML pipeline architecture examples, such as single leader architecture, directed acyclic graphs (DAGs), foreach pattern, synchronous training, parameter server architecture, and Ring-AllReduce architecture. Each pattern is explained with its advantages, disadvantages, and real-world applications, providing a comprehensive overview of ML pipeline architectures in practice.\n---\n\n# Details\n\n Two terms are often used interchangeably, yet they hold distinct meanings.\n- ML pipeline architecture is like the high-level musical score for the symphony. It outlines the components, stages, and workflows within the ML pipeline.\n- In contrast, ML pipeline design is a deep dive into the composition of the ML pipeline, dealing with the tools, paradigms, techniques, and programming languages used to implement the pipeline and its components.\n\nFour reasons for following ML pipeline architecture and designs:\n- Efficiency\n- Scalability\n- Templating and reproducibility\n- Standardization\n\nMLOps: monitoring, maintaining, and deployment stages\n\nMain stages within ML pipelines:\n- Data Ingestion (e.g., Apache Kafka, Amazon Kinesis)\n- Data Preprocessing (e.g., pandas, NumPy)\n- Feature Engineering and Selection (e.g., Scikit-learn, Feature Tools)\n- Model Training (e.g., TensorFlow, PyTorch)\n- Model Evaluation (e.g., Scikit-learn, MLflow)\n- Model Deployment (e.g., TensorFlow Serving, TFX)\n- Monitoring and Maintenance (e.g., Prometheus, Grafana)\n# 10 ML pipeline architecture examples\n\n# Single leader architecture\n\n- Master-slave architecture\n- Pros: Scale horizontally for read operations\n- Cons: Scale vertically for write operations\n\n# Directed acyclic graphs (DAG)\n\nDirected graphs are made up of nodes, edges, and directions. The nodes represent processes; edges in graphs depict relationships between processes, and the direction of the edges signifies the flow of process execution or data/signal transfer within the graph.\nFor instance, a condition in graphs where loops between vertices or nodes are disallowed. This type of graph is called an acyclic graph\nDAGs can easily be sorted topologically.\nUsing DAGs provides an efficient way to execute processes and tasks in various applications, including big data analytics, machine learning, and artificial intelligence, where task dependencies and the order of execution are crucial.\nIn modern machine learning pipelines that are expected to be adaptable and operate within dynamic environments or workflows, DAGs are unsuitable for modelling and managing these systems or pipelines, primarily because DAGs are ideal for static workflows with predefined dependencies. \n# Foreach pattern\n\nThe foreach pattern is a code execution paradigm that iteratively executes a piece of code for the number of times an item appears within a collection or set of data.\nFor example, the foreach pattern can be used in the model training stage of ML pipelines, where a model is repeatedly exposed to different partitions of the dataset for training and others for testing over a specified amount of time.\n[Image](No URL)\n\n\nUtilizing the DAG architecture and foreach pattern in an ML pipeline enables a robust, scalable, and manageable ML pipeline solution. \nBelow is an illustration of an ML pipeline leveraging DAG and foreach pattern. The flowchart represents a machine learning pipeline where each stage (Data Collection, Data Preprocessing, Feature Extraction, Model Training, Model Validation, and Prediction Generation) is represented as a Directed Acyclic Graph (DAG) node. Within each stage, the “foreach” pattern is used to apply a specific operation to each item in a collection:\n[Image](No URL)\nBut there are some disadvantages to it as well.\nWhen utilizing the foreach pattern in data or feature processing stages, all data must be loaded into memory before the operations can be executed. This can lead to poor computational performance, mainly when processing large volumes of data that may exceed available memory resources. For instance, in a use-case where the dataset is several terabytes large, the system may run out of memory, slow down, or even crash if it attempts to load all the data simultaneously.\nAnother limitation of the foreach pattern lies in the execution order of elements within a data collection. The foreach pattern does not guarantee a consistent order of execution or order in the same form the data was loaded. Inconsistent order of execution within foreach patterns can be problematic in scenarios where the sequence in which data or features are processed is significant (e.g., time-series)\n# Synchronous training\n\nIn this context, synchronous training involves a coordinated effort among all independent computational units, referred to as ‘workers’. Each worker holds a partition of the model and updates its parameters using its portion of the evenly distributed data. \n[Image](No URL)\n\nSynchronous Training is relevant to scenarios or use cases where there is a need for even distribution of training data across compute resources, uniform computational capacity across all resources, and low latency communication between these independent resources. \nCompared to asynchronous methods, synchronous training often achieves superior results as workers’ synchronized and uniform operation reduces variance in parameter updates at each step.\nSynchronous training may pose time efficiency issues as it requires the completion of tasks by all workers before proceeding to the next step. \n# Parameter server architecture\n\nThis architecture operates on the principle of server-client relationships, where the client nodes, referred to as ‘workers’, are assigned specific tasks such as handling data, managing model partitions, and executing defined operations.\nOn the other hand, the server node plays a central role in managing and aggregating the updated model parameters and is also responsible for communicating these updates to the client nodes.\n[Image](No URL)\n# Ring-AllReduce architecture\n\nThe workers independently compute their gradients during backward propagation on their own partition of the training data. A ring-like structure is applied to ensure each worker on a device has a model with parameters that include the gradient updates made on all other independent workers.\nhis is achieved by passing the sum of gradients from one worker to the next worker in the ring, which then adds its own computed gradient to the sum and passes it on to the following worker. This process is repeated until all the workers have the complete sum of the gradients aggregated from all workers in the ring.\n[Image](No URL)\nReal-world applications involving distributed machine learning training, particularly in scenarios requiring handling extensive datasets (e.g., Meta, Google)\n\n- It enables effective data parallelism by ensuring optimal utilization of computational resources. Each worker node holds a complete copy of the model and is responsible for training on its subset of the data. \n- Another advantage of Ring-AllReduce is that it allows for the aggregation of model parameter updates across multiple devices.\n# Others\n\n- Embeddings\n- Data parallelism\n- Model parallelism\n- Federated learning\n</child_page>\n\n\n\n---\n\n# Community\n\n[https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/](https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/)\n[https://www.youtube.com/watch?v=gxw1gMYP0WM&t=1171s](https://www.youtube.com/watch?v=gxw1gMYP0WM&t=1171s)\n[From MLOps to ML Systems with Feature/Training/Inference Pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)\n[MLOps: Continuous delivery and automation pipelines in machine learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)\n[Building an End-to-End MLOps Pipeline with Open-Source Tools](https://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-with-open-source-tools-d8bacbf4184f)\n[CI/CD for Machine Learning in 2024: Best Practices to Build, Train, and Deploy](https://medium.com/infer-qwak/ci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2)\nThe FTI Pipeline Architecture & Feature Store series:\n\t- [Modularity and Composability for AI Systems with AI Pipelines and Shared Storage](https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage)\n\t- [A Taxonomy for Data Transformations in AI Systems](https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems)\n\n# Papers\n\n\t- [Practitioners guide to MLOps: A framework for continuous delivery and automation of machine learning.](https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf)\n\t- [The Hopsworks Feature Store for Machine Learning](https://dl.acm.org/doi/10.1145/3626246.3653389)\n\n# Blogs\n\n\t- [https://www.hopsworks.ai/blog](https://www.hopsworks.ai/blog)\n\t- [https://www.qwak.com/blog](https://www.qwak.com/blog)\n\t- [https://www.featurestore.org/](https://www.featurestore.org/)\n\n# Courses\n\n\t- [https://github.com/DataTalksClub/mlops-zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)\n\t- [https://github.com/GokuMohandas/Made-With-ML](https://github.com/GokuMohandas/Made-With-ML)",
    "urls": [
        "https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning/",
        "https://nexocode.com/blog/posts/lambda-vs-kappa-architecture/",
        "https://www.featurestore.org/",
        "https://github.com/GokuMohandas/Made-With-ML/",
        "https://www.qwak.com/blog/",
        "https://github.com/DataTalksClub/mlops-zoomcamp/",
        "https://www.youtube.com/watch?v=gxw1gMYP0WM&t=1171s/",
        "https://www.hopsworks.ai/post/a-taxonomy-for-data-transformations-in-ai-systems/",
        "https://www.hopsworks.ai/post/modularity-and-composability-for-ai-systems-with-ai-pipelines-and-shared-storage/",
        "https://medium.com/infer-qwak/building-an-end-to-end-mlops-pipeline-with-open-source-tools-d8bacbf4184f/",
        "https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines/",
        "https://neptune.ai/blog/ml-pipeline-architecture-design-patterns/",
        "https://medium.com/infer-qwak/ci-cd-for-machine-learning-in-2024-best-practices-to-build-test-and-deploy-c4ad869824d2/",
        "https://www.hopsworks.ai/blog/",
        "https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf/",
        "https://dl.acm.org/doi/10.1145/3626246.3653389/"
    ]
}