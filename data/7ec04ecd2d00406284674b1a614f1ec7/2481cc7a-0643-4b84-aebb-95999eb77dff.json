{
    "page_metadata": {
        "id": "1dd0f09c75e443ba060cffb9fcf060b6",
        "url": "https://www.notion.so/Training-Fine-tuning-LLMs-1dd0f09c75e443ba060cffb9fcf060b6",
        "title": "Training & Fine-tuning LLMs",
        "properties": {
            "Type": "Leaf"
        }
    },
    "content": "# Resources [Community]\n\n\t- [Number of samples for fine-tuning based on general, domain, task specific](https://www.linkedin.com/posts/maxime-labonne_the-term-fine-tuning-includes-completely-activity-7267499732669796352-AwSE?utm_source=share&utm_medium=member_desktop)\n\n# Tools\n\n\t[Link Preview](https://github.com/unslothai/unsloth)\n\t[Link Preview](https://github.com/axolotl-ai-cloud/axolotl)\n\t[Link Preview](https://github.com/huggingface/trl)\n\n---\n\n# Articles\n\n\t- Parameter efficient fine-tuning: [https://lightning.ai/pages/community/article/understanding-llama-adapters/](https://lightning.ai/pages/community/article/understanding-llama-adapters/)\n\t- [Fine-tune Llama 3 with ORPO](https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada)\n\t- [LLM Fine-Tuning and Model Selection Using Neptune and Transformers](https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers)\n\n# Papers\n\n\t- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n\t- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n\n# Code & Repositories\n\n\t- [Templates for Chat Models [HuggingFace]](https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use)\n\t- [https://github.com/huggingface/trl](https://github.com/huggingface/trl)\n\n# Benchmarks\n\n\t- [MMLU (Massive Multitask Language Understanding)](/2481cc7a06434b84aebb95999eb77dff)\n\t- [Code Generation on HumanEval](https://paperswithcode.com/sota/code-generation-on-humaneval)\n\n# Videos\n\n\t- [SFT, Preference Alignment, Merging, MoE](https://www.youtube.com/live/R0X7mPagRiE?app=desktop&t=11716)",
    "urls": [
        "https://github.com/huggingface/trl/",
        "https://lightning.ai/pages/community/article/understanding-llama-adapters/",
        "https://github.com/axolotl-ai-cloud/axolotl/",
        "https://arxiv.org/abs/2305.14314/",
        "https://github.com/unslothai/unsloth/",
        "https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada/",
        "https://www.linkedin.com/posts/maxime-labonne_the-term-fine-tuning-includes-completely-activity-7267499732669796352-AwSE?utm_source=share&utm_medium=member_desktop/",
        "https://www.youtube.com/live/R0X7mPagRiE?app=desktop&t=11716/",
        "https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use/",
        "/2481cc7a06434b84aebb95999eb77dff/",
        "https://neptune.ai/blog/llm-fine-tuning-and-model-selection-with-neptune-transformers/",
        "https://arxiv.org/abs/2106.09685/",
        "https://paperswithcode.com/sota/code-generation-on-humaneval/"
    ]
}