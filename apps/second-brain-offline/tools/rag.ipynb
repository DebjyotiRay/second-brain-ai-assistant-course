{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from second_brain.config import settings\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb.retrievers import (\n",
    "    MongoDBAtlasParentDocumentRetriever,\n",
    ")\n",
    "\n",
    "from second_brain.application.rag import get_splitter\n",
    "from second_brain.application.rag.embeddings import EmbeddingModelBuilder\n",
    "\n",
    "embedding_model = EmbeddingModelBuilder().get_model()\n",
    "parent_doc_retriever = MongoDBAtlasParentDocumentRetriever.from_connection_string(\n",
    "    connection_string=settings.MONGODB_URI,\n",
    "    embedding_model=embedding_model,\n",
    "    child_splitter=get_splitter(200),\n",
    "    parent_splitter=get_splitter(800),\n",
    "    database_name=settings.MONGODB_DATABASE_NAME,\n",
    "    collection_name=\"rag\",\n",
    "    text_key=\"page_content\",\n",
    "    search_kwargs={\"k\": 10},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Retrieve and parse documents\n",
    "retrieve = {\n",
    "    \"context\": parent_doc_retriever\n",
    "    | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs])),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "}\n",
    "template = \"\"\"Answer the question based only on the following context. If no context is provided, respond with I DON'T KNOW: \\\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "# Define the chat prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "# Define the model to be used for chat completion\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-2024-11-20\")\n",
    "# Parse output as a string\n",
    "parse_output = StrOutputParser()\n",
    "# Naive RAG chain\n",
    "rag_chain = retrieve | prompt | llm | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, you can optimize LLMs for inference by using **vllm**, which is described as a high-throughput and memory-efficient inference and serving engine for large language models (LLMs). Additionally, techniques like quantization and LoRA can be employed to reduce memory and compute resource requirements.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\"How can I optimize LLMs for inference?\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF, or Reinforcement Learning from Human Feedback, is a technique used to fine-tune machine learning models, particularly language models (LMs), by incorporating human feedback. It involves training a reward model based on human annotations, which can include human-generated text or labels of human preferences between model outputs. This reward model is then used to guide the optimization of the language model's behavior, often through reinforcement learning algorithms like PPO (Proximal Policy Optimization). RLHF aims to align model outputs with human preferences, but it is resource-intensive and depends heavily on the quality of human annotations.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\"What is RLHF?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Recommenders (TFRS) is a library designed to simplify the development of recommendation systems. It allows developers to build custom models, such as two-tower architectures, for deep retrieval tasks. In the two-tower setup, one neural network tower generates embeddings for queries, while another tower generates embeddings for candidate items. These embeddings are mapped to a shared embedding space, where the similarity between a query and a candidate is determined by calculating the dot product of their embeddings. This approach enables efficient and scalable candidate retrieval by precomputing candidate embeddings and focusing on query embedding computation and similarity search during serving.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\"How does Tensorflow Recommenders work?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ----------------------------------------------------------------------------------------------------\n",
      "[Contact sales ](https://cloud.google.com/contact/)[Get started for free ](https://console.cloud.goo\n",
      "1    ----------------------------------------------------------------------------------------------------\n",
      "## Background\n",
      "\n",
      "To meet low latency serving requirements, large-scale recommenders are often deployed\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(parent_doc_retriever.invoke(\"How does Tensorflow Recommenders work?\")):\n",
    "    print(i, \"  \", \"-\" * 100)\n",
    "    print(doc.page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ----------------------------------------------------------------------------------------------------\n",
      "Share\n",
      "\n",
      "![Advanced Retrieval-Augmented Generation \\(RAG\\) implements pre-retrieval, retrieval, and po\n",
      "1    ----------------------------------------------------------------------------------------------------\n",
      "[![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='1200'%20width='2400'%20xmlns='http://www.w3.o\n",
      "2    ----------------------------------------------------------------------------------------------------\n",
      "[![](data:image/svg+xml;charset=utf-8,%3Csvg%20height='670'%20width='1200'%20xmlns='http://www.w3.or\n",
      "3    ----------------------------------------------------------------------------------------------------\n",
      "Image By Author\n",
      "\n",
      "Retrieval Augmented Generation (RAG) has been around for a while, taking many forms\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(parent_doc_retriever.invoke(\"What is RAGAS?\")):\n",
    "    print(i, \"  \", \"-\" * 100)\n",
    "    print(doc.page_content[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
