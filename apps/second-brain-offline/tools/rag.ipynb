{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-18 10:56:28.616\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msecond_brain.infrastructure.mongo.service\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m54\u001b[0m - \u001b[1mConnected to MongoDB instance:\n",
      " URI: mongodb://decodingml:decodingml@localhost:27017/?directConnection=true\n",
      " Database: second_brain\n",
      " Collection: mongodb://decodingml:decodingml@localhost:27017/?directConnection=true\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from second_brain.config import settings\n",
    "from second_brain.infrastructure.mongo import MongoDBService\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "\n",
    "mongodb_client = MongoDBService(settings.MONGODB_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mongodb.retrievers import (\n",
    "    MongoDBAtlasParentDocumentRetriever,\n",
    ")\n",
    "from second_brain.application.rag import get_splitter\n",
    "from second_brain.application.rag.embeddings import EmbeddingModelBuilder\n",
    "\n",
    "embedding_model = EmbeddingModelBuilder().get_model()\n",
    "parent_doc_retriever = MongoDBAtlasParentDocumentRetriever.from_connection_string(\n",
    "    connection_string=settings.MONGODB_URI,\n",
    "    embedding_model=embedding_model,\n",
    "    child_splitter=get_splitter(200),\n",
    "    parent_splitter=get_splitter(800),\n",
    "    database_name=settings.MONGODB_DATABASE_NAME,\n",
    "    collection_name=\"rag\",\n",
    "    text_key=\"page_content\",\n",
    "    search_kwargs={\"k\": 10},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Retrieve and parse documents\n",
    "retrieve = {\n",
    "    \"context\": parent_doc_retriever\n",
    "    | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs])),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "}\n",
    "template = \"\"\"Answer the question based only on the following context. If no context is provided, respond with I DON'T KNOW: \\\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "# Define the chat prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "# Define the model to be used for chat completion\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-2024-11-20\")\n",
    "# Parse output as a string\n",
    "parse_output = StrOutputParser()\n",
    "# Naive RAG chain\n",
    "rag_chain = retrieve | prompt | llm | parse_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To optimize LLMs for inference, you can use the following techniques:\n",
      "\n",
      "1. **Lower Precision (Quantization)**:\n",
      "   - Use 8-bit or 4-bit precision to reduce memory usage and computational requirements without significant performance loss.\n",
      "\n",
      "2. **Flash Attention**:\n",
      "   - Implement Flash Attention for faster and more memory-efficient inference by utilizing on-chip memory (SRAM) instead of slower GPU VRAM.\n",
      "\n",
      "3. **Speculative Decoding**:\n",
      "   - Use a smaller model to generate draft tokens and a larger model to verify them, reducing latency, memory usage, and compute demands.\n",
      "\n",
      "4. **Caching**:\n",
      "   - Implement KV-caching or prompt caching to reuse computations and speed up inference.\n",
      "\n",
      "5. **Compilers**:\n",
      "   - Use tools like `torch.compile()` or TensorRT to optimize model execution.\n",
      "\n",
      "6. **Continuous Batching**:\n",
      "   - Dynamically batch requests to maximize GPU utilization.\n",
      "\n",
      "7. **Optimized Attention Mechanisms**:\n",
      "   - Use techniques like PagedAttention or FlashAttention for efficient attention computation.\n",
      "\n",
      "8. **Model Parallelism**:\n",
      "   - Employ data, pipeline, or tensor parallelism to distribute computation and reduce VRAM or latency requirements.\n",
      "\n",
      "9. **Quantization Techniques**:\n",
      "   - Use formats like GGUF for CPU optimization or GPTQ, EXL2, and AWQ for GPU optimization.\n",
      "\n",
      "These techniques can significantly improve the speed, memory efficiency, and overall performance of LLMs during inference.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\"How can I optimize LLMs for inference?\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
